{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/michele/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/michele/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/michele/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/michele/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/michele/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/michele/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import random\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.data import load\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from typing import Tuple, List, Any, Dict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#import seaborn as sn\n",
    "#import pandas as pd\n",
    "\n",
    "\n",
    "#seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#setting up nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"tagsets\")\n",
    "stop_tokens = set(stopwords.words('english'))\n",
    "punc_tokens = set(punctuation)\n",
    "stop_tokens.update(punc_tokens)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#setting the embedding dimension\n",
    "EMBEDDING_DIM=100\n",
    "POS_EMBEDDING_DIM=10\n",
    "\n",
    "#specify the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "#setting unknown token to handle out of vocabulary words and padding token to pad sentences\n",
    "UNK_TOKEN = '<unk>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "EN_TRAIN_PATH = \"./../../data/EN/train.json\"\n",
    "EN_DEV_PATH = \"./../../data/EN/dev.json\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 123>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    119\u001B[0m             converted\u001B[38;5;241m.\u001B[39mappend(converted_sent)\n\u001B[1;32m    120\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m converted\n\u001B[0;32m--> 123\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mSentenceDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEN_TRAIN_PATH\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36mSentenceDataset.__init__\u001B[0;34m(self, vectors, word2idx, pos_vectors, pos2idx, class2id, sentences_path, sentences, lemmatization, test)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m,vectors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,word2idx\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,pos_vectors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,pos2idx\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,class2id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,sentences_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,sentences\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,lemmatization\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m      4\u001B[0m              test\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m----> 5\u001B[0m     file_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences_path\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m sentences_path \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_sentences(sentences)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(file_output)\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;03m    self.embedding_vectors = vectors\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;03m    self.word2idx = word2idx\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124;03m    self.id2class = {v: k for (k, v) in self.class2id.items()}\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36mSentenceDataset.read_file\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m     27\u001B[0m roles \u001B[38;5;241m=\u001B[39m json_file[key][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroles\u001B[39m\u001B[38;5;124m\"\u001B[39m][position]\n\u001B[1;32m     28\u001B[0m predicates \u001B[38;5;241m=\u001B[39m [_ \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(roles))]\n\u001B[0;32m---> 29\u001B[0m predicates[position] \u001B[38;5;241m=\u001B[39m \u001B[43mjson_file\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpredicates\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mposition\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     30\u001B[0m instance \u001B[38;5;241m=\u001B[39m json_file[key]\n\u001B[1;32m     31\u001B[0m instance[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroles\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m roles\n",
      "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self,vectors=None,word2idx=None,pos_vectors=None,pos2idx=None,class2id=None,sentences_path=None,sentences=None,lemmatization=False,\n",
    "                 test=False):\n",
    "        file_output = self.read_file(sentences_path) if sentences_path else self.read_sentences(sentences)\n",
    "        print(file_output)\n",
    "        '''\n",
    "        self.embedding_vectors = vectors\n",
    "        self.word2idx = word2idx\n",
    "        self.pos_vectors = pos_vectors\n",
    "        self.pos2idx = pos2idx\n",
    "        self.test = test\n",
    "        self.w_lemmatization = lemmatization\n",
    "        self.extract_sentences(file_output)\n",
    "        self.class2id = class2id\n",
    "        self.id2class = {v: k for (k, v) in self.class2id.items()}\n",
    "        '''\n",
    "\n",
    "    #little function to read and store a file given the path\n",
    "    def read_file(self,path):\n",
    "        sentences = list()\n",
    "        with open(path) as file:\n",
    "            json_file = json.load(file)\n",
    "            for key in json_file:\n",
    "                if len(json_file[key][\"roles\"]) > 1:\n",
    "                    for position in json_file[key][\"roles\"]:\n",
    "                        roles = json_file[key][\"roles\"][position]\n",
    "                        predicates = [_ for i in range(len(roles))]\n",
    "                        predicates[position] = json_file[key][\"predicates\"][position]\n",
    "                        instance = json_file[key]\n",
    "                        instance[\"roles\"] = roles\n",
    "                        instance[\"predicates\"] = predicates\n",
    "                        sentences.append(instance)\n",
    "                else:\n",
    "                    instance = json_file[key]\n",
    "                    key = instance[\"roles\"].keys()[0]\n",
    "                    instance[\"roles\"] = instance[\"roles\"][key]\n",
    "                    sentences.append(instance)\n",
    "        return sentences\n",
    "\n",
    "    #little function to read and store a set of sentences given as a list of tokens\n",
    "    def read_sentences(self,sentences):\n",
    "        sents = list()\n",
    "        for idx,line in enumerate(sentences):\n",
    "            d = dict()\n",
    "            d[\"id\"] = idx\n",
    "            d[\"text\"] = line\n",
    "            d[\"labels\"] = [\"O\" for token in line]\n",
    "            sents.append(d)\n",
    "        return sents\n",
    "\n",
    "    #function to extract the sentences from the dictionary of samples\n",
    "    def extract_sentences(self,file_output):\n",
    "        self.sentences = list()                 #creating a list to store the instances in the dataset\n",
    "        for instance in file_output:\n",
    "            processed = self.text_preprocess(instance)      #process every sample (sentence) with the text_preprocess function\n",
    "            labels = 'UNKNOWN'   #this is needed to make the system able to store the sentences without a ground truth (for predictions)\n",
    "            if 'labels' in instance: #but if there is a ground truth we take it\n",
    "                labels = processed['labels']\n",
    "            self.sentences.append((processed[\"text\"],processed[\"pos\"], labels, id))           #append a tuple (sentence,pos,labels,id) which are all the informations we need\n",
    "        if not self.test: random.Random(42).shuffle(self.sentences)         #for the training phase, shuffle data to avoid bias relative to data order\n",
    "\n",
    "    #function to convert the pos extracted by nltk to the pos required by the very same library for lemmatization\n",
    "    #I also use it to give pos='' to punctuation\n",
    "    def get_standard(self,pos):\n",
    "        if pos[0] == 'V': return wordnet.VERB\n",
    "        if pos[0] == 'R': return wordnet.ADV\n",
    "        if pos[0] == 'N': return wordnet.NOUN\n",
    "        if pos[0] == 'J': return wordnet.ADJ\n",
    "        return ''\n",
    "\n",
    "    #function for preprocessing, which includes pos tagging and (if specified) lemmatization\n",
    "    def text_preprocess(self,sentence):\n",
    "        tokens_n_pos = nltk.pos_tag(sentence[\"text\"])\n",
    "        standard_tokens = [(token,self.get_standard(pos)) for token,pos in tokens_n_pos]\n",
    "        if self.w_lemmatization:            #choosing if applying lemmatization\n",
    "            lemmatized = [(lemmatizer.lemmatize(token.lower(),pos),pos) if pos != '' else (lemmatizer.lemmatize(token.lower()),'') for token,pos in standard_tokens]\n",
    "            sentence[\"text\"] = [lemma for lemma,pos in lemmatized]\n",
    "        sentence[\"pos\"] = [pos for word,pos in standard_tokens]\n",
    "        return sentence\n",
    "\n",
    "    #function to return the number of instances contained in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    #function to get the i-th instance contained in the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "    #custom dataloader which incorporates the collate function\n",
    "    def dataloader(self,batch_size):\n",
    "        return DataLoader(self,batch_size=batch_size,collate_fn=partial(self.collate))\n",
    "\n",
    "    #function to map each lemma,pos in a sentence to their indexes\n",
    "    def sent2idx(self ,sent, word2idx):\n",
    "        return torch.tensor([word2idx[word] for word in sent])\n",
    "\n",
    "    #custom collate function, used to create the batches to give as input to the nn\n",
    "    #it's needed because we are dealing with sentences of variable length and we need padding\n",
    "    #to be sure that each sentence in a batch has the same length, which is necessary\n",
    "    def collate(self, data):\n",
    "        X = [self.sent2idx(instance[0], self.word2idx) for instance in data]                            #extracting the input sentence\n",
    "        X_len = torch.tensor([x.size(0) for x in X], dtype=torch.long).to(device)                       #extracting the length for each sentence\n",
    "        X_pos = [self.sent2idx(instance[1], self.pos2idx) for instance in data]                         #extracting pos tags for each sentence\n",
    "        y = [self.sent2idx(instance[2], self.class2id) for instance in data]                            #extracting labels for each sentence\n",
    "        ids = [instance[3] for instance in data]                                                        #extracting the sentences' ids\n",
    "        X = torch.nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=1).to(device)            #padding all the sentences to the maximum length in the batch (forcefully max_len)\n",
    "        X_pos = torch.nn.utils.rnn.pad_sequence(X_pos, batch_first=True, padding_value=1).to(device)    #padding all the pos tags\n",
    "        y = torch.nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=self.class2id[PAD_TOKEN]).to(device)              #padding all the labels\n",
    "        return X, X_len,X_pos,y, ids\n",
    "\n",
    "    #function to convert the output ids to the corresponding labels\n",
    "    def convert_output(self,output):\n",
    "        converted = []\n",
    "        for sentence in output:\n",
    "            converted_sent = []\n",
    "            for label in sentence:\n",
    "                converted_sent.append(self.id2class[label.item()])\n",
    "            converted.append(converted_sent)\n",
    "        return converted\n",
    "\n",
    "\n",
    "dataset = SentenceDataset(sentences_path=EN_TRAIN_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StudentModel(Model):\n",
    "\n",
    "    # STUDENT: construct here your model\n",
    "    # this class should be loading your weights and vocabulary\n",
    "    # MANDATORY to load the weights that can handle the given language\n",
    "    # possible languages: [\"EN\", \"FR\", \"ES\"]\n",
    "    # REMINDER: EN is mandatory the others are extras\n",
    "    def __int__(self, language: str):\n",
    "        # load the specific model for the input language\n",
    "        self.language = language\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        \"\"\"\n",
    "        --> !!! STUDENT: implement here your predict function !!! <--\n",
    "\n",
    "        Args:\n",
    "            sentence: a dictionary that represents an input sentence, for example:\n",
    "                - If you are doing argument identification + argument classification:\n",
    "                    {\n",
    "                        \"words\":\n",
    "                            [  \"In\",  \"any\",  \"event\",  \",\",  \"Mr.\",  \"Englund\",  \"and\",  \"many\",  \"others\",  \"say\",  \"that\",  \"the\",  \"easy\",  \"gains\",  \"in\",  \"narrowing\",  \"the\",  \"trade\",  \"gap\",  \"have\",  \"already\",  \"been\",  \"made\",  \".\"  ]\n",
    "                        \"lemmas\":\n",
    "                            [\"in\", \"any\", \"event\", \",\", \"mr.\", \"englund\", \"and\", \"many\", \"others\", \"say\", \"that\", \"the\", \"easy\", \"gain\", \"in\", \"narrow\", \"the\", \"trade\", \"gap\", \"have\", \"already\", \"be\", \"make\",  \".\"],\n",
    "                        \"predicates\":\n",
    "                            [\"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"AFFIRM\", \"_\", \"_\", \"_\", \"_\", \"_\", \"REDUCE_DIMINISH\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"MOUNT_ASSEMBLE_PRODUCE\", \"_\" ],\n",
    "                    },\n",
    "                - If you are doing predicate disambiguation + argument identification + argument classification:\n",
    "                    {\n",
    "                        \"words\": [...], # SAME AS BEFORE\n",
    "                        \"lemmas\": [...], # SAME AS BEFORE\n",
    "                        \"predicates\":\n",
    "                            [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0 ],\n",
    "                    },\n",
    "                - If you are doing predicate identification + predicate disambiguation + argument identification + argument classification:\n",
    "                    {\n",
    "                        \"words\": [...], # SAME AS BEFORE\n",
    "                        \"lemmas\": [...], # SAME AS BEFORE\n",
    "                        # NOTE: you do NOT have a \"predicates\" field here.\n",
    "                    },\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with your predictions:\n",
    "                - If you are doing argument identification + argument classification:\n",
    "                    {\n",
    "                        \"roles\": list of lists, # A list of roles for each predicate in the sentence.\n",
    "                    }\n",
    "                - If you are doing predicate disambiguation + argument identification + argument classification:\n",
    "                    {\n",
    "                        \"predicates\": list, # A list with your predicted predicate senses, one for each token in the input sentence.\n",
    "                        \"roles\": dictionary of lists, # A list of roles for each pre-identified predicate (index) in the sentence.\n",
    "                    }\n",
    "                - If you are doing predicate identification + predicate disambiguation + argument identification + argument classification:\n",
    "                    {\n",
    "                        \"predicates\": list, # A list of predicate senses, one for each token in the sentence, null (\"_\") included.\n",
    "                        \"roles\": dictionary of lists, # A list of roles for each predicate (index) you identify in the sentence.\n",
    "                    }\n",
    "        \"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}